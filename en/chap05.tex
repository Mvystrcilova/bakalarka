\chapter{Experiments}\label{chap:experiments}

In this chapter we describe how the chosen song encoding methods were implemented. This includes describing the input, training (if there was any) and output (meaning the vector-encoded representations for each song in our 16594 songs dataset). We also acquaint the reader with our evaluation methods, the reason we decided to use them and the evaluation results for each method. The results are first presented separately for each method (or a group of closely related methods) and then summarized and most importantly interpreted in the \ref{sec:discussion} where various graphs and tables illustrating the prominent or interesting trends and tendencies can be found. Before all this however, we start with stating what the expected outcomes were before we even started.

\section{Expectations}

Even before reading any literature, we made two main predictions. 
\begin{itemize}
    \item \textbf{First:} Audio-based methods will perform better than text-based methods.
    \item \textbf{Second:} More advanced methods will outperform simpler machine learning methods.
\end{itemize}
\todo[inline]{Pridal bych dalsi expectation, ze slozitejsi metody jsou obvykle vice time-consuming a snazil se tim padem najit nejaky tradeoff mezi vysledky a narocnosti. Zkusilas premyslet nad tim proc sis myslela, ze audio budou lepsi nez text? Mohlo by to treba souviset s mnozstvim ulozene informace? Dal by bylo docela dobre pridat jake metody povazujes za "more advanced"}
These were based on our intuition and later also supported by reading into this topic, where most of the papers we studied were describing neural networks performing similar audio or text-based recommendation or classification and their results were compared to simpler algorithms. \\
We were also hoping, that the text-based methods will not be complete irrelevant. In this case, we were right, but only to the extend that the text-based methods are not irrelevant in comparison to the audio-based methods. Overall, all the methods have shown that they seem to lack the properties to be useful as methods for recommendation.

\todo[inline]{Tady celkove, nakolik jsem pochopil (zrovna tahle cast textu je dost v mlze) michas dohromady expectations se zavery vyhodnoceni. Asi bych tady popsal, klidne detailneji duvody ktere nas k tem predpokladum vedly, ale hodnoceni nechal az na dalsi kapitoly. Zase by to melo sledovat nejakou jednoduchou casovou linku Predpoklady -> sestaveni experimentu -> vysledky -> diskuze (a potvrzeni/vyvraceni predpokladu / hypotez)}

\section{Evaluation}

\subsection{Wanted LP:Desired recommender-system features}
We already touched this in the introduction but it is useful to revise what we want from a good recommendation system and what its most important features are. Let's skip the software part for now (as it will be discussed further in the \textbf{Web application} chapter and focus purely on what it recommends. Probably the most crucial property a recommendation system should have is that it should include the items the user actually likes between the first 10 to maybe 50 recommendations. Because it does not really matter if an item the user would like ends up on the 500th or 5000th position. People rarely go that deep. \\
Another thing we would want is for the system to be able to improve its recommendations with a rising amount of data it acquires about a user. In our case this means, that we would expect the predictions to be better for users with longer playlists. \\
Even though the whole idea of this thesis is to approach recommendation more loosely, we still want our methods to posses these features to at least some reasonable extend. Also, as these are the properties other recommendation systems are being evaluated on, we can gain a better understanding of the features our methods share with other recommendation techniques as well as their where they differ. 
\subsection{Evaluation measures}\label{ssec:evaluation_measures}
To test the wanted features described in the section above we performed evaluation as follows. \\
Our dataset contains 11123 playlists that were used to evaluate our algorithms. We chose to evaluate our algorithms only on playlists of length at least four. For each method, we did a 5-cross-validation where in a validation epoch, every playlists $p_i$ was divided into two parts a training part $p_{i_{train}}$ and a testing part $p_{i_{test}}$ with an approximately 80:20 ratio. A higher priority was set on the fact that the test part always had to contain at least one entry (meaning that for playlists of length 2, the ratio would be 50:50). \\
Afterwards for each song $ s_k $ from our song dataset $S$, the similarity of the whole $p_{i_{train}}$ to $s_k$ which we denote as $ sim(p_i, s_k) $ was calculated as $$ sim(p_i, s_k) =\sum_{s_j\in{p_i{_{train}}}} cos\_sim(s_i, s_j) $$ where $ i \neq j$ and $cos\_sim$ is the cosine similarity. These similarities were then sorted and it was determined at what position the songs from $p_{i_{test}} $ that actually belong to the playlist came. \\
These positions were then used to calculate multiple evaluation measures used to assess how well does each algorithm predict the missing part of a users playlist. For this we chose the following for methods:
\begin{itemize}
    \item Recall at 10 (= \textbf{R@10}) defined as the number of songs from $p_{i_{test}} $ that placed in the top ten most similar songs.
    \item Recall at 50 (= \textbf{R@50}) defined as the number of songs from $p_{i_{test}} $ that placed in the top fifty most similar songs.
    \item Recall at 100 ( = \textbf{R@100} ) defined as the number of songs from $p_{i_{test}} $ that placed in the top hundred most similar songs.
    \item Normalized cumulative discounted gain (= \textbf{nCDG}) defined as 
    $${nDCG_{r}} = \frac{DCG_{r}}{IDCG{r}} $$
    where 
    $${DCG_{r}} =\sum_{i=1}^{r}{\frac {rel_{i}}{\log _{2}(i+1)}} $$ 
    is the discounted cumulative gain at position r and 
    $$ {IDCG_{r}} =\sum _{i=1}^{|REL|}{\frac {2^{rel_{i}}-1}{\log _{2}(i+1)}} $$
    is the ideal discounted cumulative gain at r
    where $r$ is in our case the number of songs that had to be predicted, the $rel_i$, meaning relevance, is the same for all songs as all songs in one playlists have the same relevance and $|REL|$ is the list of relevant items (in our case the songs from $p_{test}$).
    \item Average rank of a song from the $p_{i_{test}}$ set $ \boldsymbol{ (= \overline{rank})} $ which we included to have also an indication of the overall behaviour, not only the first 100 ranks.
    \item A graph plotting the distribution of rankings of songs from the test part of each playlists which we called the \textit{RDG} (=\textit{rank distribution graph}). We summed the number of songs from $p_{test}$ to which each individual rank from 1 to 16594 and divided it by the number of all songs that were in a testing part of some playlist. So for example if we had two playlists both with two songs from $p_{test}$ and our method assigned ranks 30 and 2900 to the $p_{test}$ songs from the first playlist and 2900 and 4872 to those from the second playlists, our graph would plot the values 0.25 for rank 30, 0.5 for rank 2900 and 0.25 for rank 4872 and 0 for the rest of the ranks. We did this for all playlists together but we also plotted the distributions for chosen playlist lengths separately to see if our predictions improve for longer playlists as we were hoping for. The x-scale of the \textit{RDG} was scaled logarithmicaly as we are much more interested in what is going on on the first 100 positions than what is goint on in the middle or towards the end. \\
\end{itemize}
After running this evaluation, we noticed that for most of our similarity methods, the positions for songs from $p_{i_{test}} $ that belonged to a short playlist were higher up, than those from long playlists especially for the first three ranks. Therefore, we changed our evaluation method a little bit. We set a threshold for each similarity method, and if the similarity between two songs was smaller than this threshold, we set the similarity to 0. This means, that our new similarity function $cos\_sim_t(s_i,s_j)$ is defined as follows:
\todo[inline]{Tady neni moc dobre poznat, proc to delas - zkus zapracovat na lepsim vysvetleni. Navic bych to nenazval vyhodnocovani. Ty pouzivas doporucovani, ktere je CB, zalozene na cosinove podobnosti a nejak agreguje vysledky z playlistu (to jsem tu ostatne zatim nikde nevidel, je treba to popsat!!). Cili zavedenim thresholdu vpodstate menis tu doporucovaci metodu, ne evaluaci, ta zustava stejna (pripomen mi tohle na osobni schuzce, jen v poznamkach se to blbe vysvetluje...)}
\todo[inline]{Figure 5.1 je strasne neprehledna - je tam toho proste moc. Misto ni bych asi reportoval tabulku s par metodama a jejich vysledcich s/bez thresholdu, pripadne prumer pres vsechny metody s/bez thresholdu, pripadne tu figure 5.1 zredukoval treba jen na 2-3 metody. Nicmene nejakou formu Figure 5.1 bych zachoval i ted - spis pro nejaky celkovy prehled metod - ale asi bych vynechal ty bez thresholdu a nejak popsal: kdo je celkovy vitez, kdo byl nejlepsi z text-based metod atd.} 
\[   
cos\_sim_t(s_i,s_j) = 
     \begin{cases}
       \text{$ cos\_sim(s_i,s_j) $} &\quad\text{if $ cos\_sim(s_i,s_j)  >= threshold$}\\
       \text{$ 0 $} &\quad\text{if $ cos\_sim(s_i,s_j)  < threshold$}\\
     \end{cases}
\]
 The threshold was chosen as $ 51 * |S| $ where $ S $ is our dataset which is 846294.\todo[inline]{Tomu nerozumim - velikost datasetu pokud vim neni skoro milion. Potom taky cos_sim je mezi 0 a 1,takze definice nedava smysl. Podivej se na to jeste jednou} The most similar song is always the song itself, so it leaves us with approximately 50 most similar songs to each song. \\
The results described in the section here are all from the evaluation with threshold. If the evaluation without threshold was better for a method, it is mentioned in the method's section. The reason to use the threshold-evaluation is also the fact, that we use the thresholds to calculate similarity in our application too. Not only because of the better results but also because of the fact, that it dramatically reduces the number of similarities that have to be stored in our database.\\
To give an idea about how the results changed with applying the threshold, lets take a look at the plots in Figure \ref{fig:threshold_method_comparison}. We plotted two lines for each method. The dashed line represents the recall values after evaluation without threshold, the continuous line represents the recall values evaluated with threshold. \\
The graph on the left shows how the \textit{recall} values changed for all our 11123 playlists. We can notice, that all methods improved except of PCA\_spec\_320, which had worse results for \textit{R@100}. The most crucial remark here however is the behaviour of short and long playlists. The results for short playlists did not improve as much and sometimes even worsened as we can see in the graph in the middle, whereas the results for long playlists improved quite a lot which is illustrated by the graph on the right. This is our desired behaviour. From the \textit{RDG} graphs later on in the specific method sections, we will see, that for short playlists, the first one to four ranks are very numerous. For the following ranks however, there is a sharp drop, which does not happen with longer playlists, definitely not as significantly. \\

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/threshold_method_ranking.png}
	\caption{The comparison of absolute max, average and min values for the evaluation measures for recommendation with and without threshold}
	\label{fig:absolute_value_comparison}
\end{figure}

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/no_threshold_method_ranking.png}
	\caption{The relative performance of each method based on how it performed for various evaluation measures when recommending without threshold. The method was assigned a rank from 1 to 16 (16 is the number of the methods in this figure) for each measure based on how it placed in that particular one. The more darker green the better was the placement, yellow are the average ones and red are the for the worst places.}
	\label{fig:no_threshol_method_comparison}
\end{figure}
\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/mean_min_max_measure_comparison.png}
	\caption{The relative performance of each method based on how it performed for various evaluation measures when recommending with threshold. The method was assigned a rank from 1 to 16 (16 is the number of the methods in this figure) for each measure based on how it placed in that particular one. The more darker green the better was the placement, yellow are the average ones and red are the for the worst places.}
	\label{fig:threshold_method_comparison}
\end{figure}
\subsubsection{Results}
    

We calculated each of the four measures (\textit{R@10, R@50, R@100} and \textit{nGDC} for each playlist and then averaged over the whole playlist dataset. Every method section contains a table with the four measure averages and a graph. Both are accompanied with a short summary of the most obvious observations.

To make things more easy for the rest of the thesis we introduce a nomenclature for our experimental methods. There are 3 different parts each of our method has. The name of the main algorithm, the name of the input and the length of the output vector. Every method can be uniquely identified by a combination of these three parts. Where there is only one or two of these necessary, we omit those which are surplus.\\
So for example a PCA method with spectrogram input and the output vector length of 320 is denoted as PCA\_spec\_320. If we take a look at the notation of the Tf-idf method, it will be only Tf-idf as there are no variations to it. However the PCA with Tf-idf input will be PCA\_Tf-idf as there is also a PCA method with mel-spectrograms as input. We separate the name parts with '\_' 

\section{Text method experiments}

\subsection{TF-idf experiments}\label{ssec:TF_idf}

\subsubsection{Input}
The lyrics for each song were stripped of all punctuation characters as well as apostrophes, converted into a single string. 
\subsubsection{Training}
Each lyric-string was appended to the whole training dataset which was then passed to an instance of \texttt{sklearn.feature\_extraction.text.TFidfVectorizer} where the method \texttt{fit\_transform} was called. The results were saved into a numpy file containing a sparse matrix. And the model was saved using pickle. \\

\subsubsection{Output}
The song representation were vectors of length 40165. They contained a lot of zeros so it was possible to store them as \textit{sparse vectors} however, is is more difficult to store a sparse vector in a database than it is with dense vectors and we would probably have to save them separately in files.

\subsubsection{Results}

The results of the TF-idf method places well above average between our methods. This was a bit of a surprise as we did not expect this "simple" method to perform well compared to others. \\

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth {| c || X[c] | X[c] | X[c] | X[c] | X[c] | }
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 Tf-idf & 0.05045 & 0.06187 & 0.06648 & 0.04214 & 7795 \\
 \hline
 PCA on Tf-idf & 0.05417 & 0.0635 & 0.06649 & 0.04371 & 7838 \\
 \hline
\end{tabu} \\
\caption{Table summarizing average TF-idf and Tf-idf with PCA values averaged over the 5 cross validation that were performed}
\label{table:1}
\end{table}

When looking at the number in \ref{table:1} we can see that 5\% of songs that were in our $p_{test}$ sets ranked in the first ten, 6.2\% in the first fifty and 6.6\% in the first hundred. The average rank of a song from our $p_{test}$ was 7795 which is quite close to the middle. We also crated a \textit{RDG} as one can see in Figure \ref{fig:tf_idf_distribution}. It appears that according to the distribution, a song is more likely to end up in the first 10-100 songs than it is at the end. One could claim that this distribution is not random and favors ranks at the beginning, however not as strongly as we would want.\\
Another thing to notice in \ref{fig:tf_idf_distribution} is that there is a general trend not only for the Tf-idf to rank a lot of songs between the first few but then drop sharply for further ranks. Longer playlists seem be more stable and as \ref{fig:method_comparison} also have better results over the whole first hundred ranks.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/tf_idf_graph.png}
  \captionof{Distribution of ranks of songs from the $ p_{test} $ set the tf-idf method assigned them.}
  \label{fig:tf_idf_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/pca_tf_idf_graph.png}
  \captionof{Distribution of ranks of songs from the $p_{test}$ for tf-idf vectors as input into PCA}
  \label{fig:pca_tf_idf_distribution}
\end{minipage}
\end{figure}

\subsection{PCA on Tf-idf}\label{ssec:PCA_on_tf_idf}

Because Tf-idf has proven to yield good results we wanted to implement it. The long vectors pose a problem to our web application though, as it is necessary to calculate the distance between a newly added song and all the songs that are already in the database which is more complex for longer vectors. Since the machine on which our application is running does not have such computational power as those we run our evaluations on, we decided to try to reduce the dimensions using PCA because it has proven to be quite effective with mel spectrograms.

\subsubsection{Input}
As input, we provided the tf-idf vectors which we acquired as described \ref{ssec:TF_idf}. We did not normalize them.

\subsubsection{Training}
For training we first run a PCA from Python's \texttt{sklearn.decomposition.PCA}. We then chose the space were the explained variance ratio was equal to 90\% which left us with 4457 features. This means, our Tf-idf vector went from 40165 to 4457 dimensions. 

\subsubsection{Output}
The output were vectors of length 4457.

\subsubsection{Results}
The results of the PCA-reduced Tf-idf vector turned out to be better then full Tf-idf vectors. As we can see in \ref{table:1} the number are better for the \textit{R10} and \texit{R50}, with the \textit{R@100} the values are almost the same. Figure \ref{fig:method_comparison} illustrates the fact, that after the application of our threshold, this method became the best one for long playlists jumping over the PCA\_mel\_5715. 

\subsection{Word2Vec experiments}\label{ssec:w2v_experiments}

\subsubsection{Input}
The input for our W2V model was the same as for the Tf-idf method.

\subsubsection{Training}
In the case of Word2Vec, we did not perform any training. Instead, we used a pre-trained W2V model from Google \todo{odkaz nebo reference!!} (the first 200000 words which covered all meaningful words in our lyric dataset). If there was a word in a song that is not in our subset added in the web application, would be ignored. Because the Google model takes always just one word as input and produces it's vector, we had to put the vectors together into one song representation. We chose a basic approach where we summed all the vectors and computed the averaged the values in each \todo{Tady to slovo, proste jeden prvek toho vektoru}

\subsubsection{Output}
The output for a word when transformed by the Google model is a vector of fixed length 300. Our resulting vector was of the same length as it only averaged over the 300 positions. This is significantly lower than the Tf-idf vector.

\subsubsection{Results}
The small size of the vectors being produced by the W2V method are a significant advantage of this method. However the results it yielded make it below average even compared to our other methods. In ought to be mentioned that even compared to methods producing vectors of similar length such as PCA\_mel\_320.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth { | c || X[c] | X[c] | X[c] | X[c] | X[c] |}
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 W2V & 0.03519 & 0.04780 & 0.05544 & 0.030313 & 7804 \\
 \hline
\end{tabu} \\
\caption{Table summarizing average W2V values averaged over the 5 cross validation that were performed}
\label{table:2}
\end{table}

The table shows lower numbers than for the Tf-idf method. 3.5\% of songs from the $ p_{test} $ set ranked in the top 10, 4.8\% in top 50 and 5.5\% in top 100. The average rank was 7804. When looking at the distribution graph, it is very clear that the gap here between the accuracy of predicted ranks for short and longer playlists is big and \ref{fig:method_comparison} where the W2V method is drawn in dark green, shows us, that even though the threshold application helped this method a lot, it still performs better for shorter playlists. 

\begin{figure}[h]
    \centering
	\includegraphics[width=120mm]{./img/w2v_graph.png}
	\caption{Distribution of ranks of songs from the test set the w2v method assigned them.}
	\label{fig:w2v_distribution}
\end{figure}

\subsection{SOM experiments}

\subsubsection{Input}
We tried the W2V vectors from \ref{ssec:w2v_experiments} as input for our self organizing map, mainly because of their length and also because we were hoping, that the SOM could improve the results of the W2V method. We also tried to train the SOM on the output vectors of our PCA\_Tf-idf method. We did not try the raw Tf-idf vectors as they are long and it would prolong training significantly. Also, it makes sense to use PCA\_Tf-idf because the it yielded better results than raw Tf-idf.

\subsubsection{Training}
We used a python library called \texttt{minisom} \cite{Vettigli2019} to create our self organizing maps. We build a map with a grid size 5 times our dataset size (16594) and the number of iterations was also 5 times our dataset size. We saved a model after each multiple of 16594 iterations and interestingly, the representations did not change after 33 187 iterations (meaning 2*16594). It was also necessary to normalize our input vectors and set learning rate to 0.2. Otherwise the songs formed 3 to 5 large clusters on the grid placing thousands of songs on the same coordinate. 
\subsubsection{Output}
The output representation for each song was a vector of length two. The whole dataset then could be displayed on a 2D map. 
When we then tried to display the map with all the songs, the size of the image would have to be immense so that all 16594 songs would be recognizable. The problem was that there were many songs to display on the map and the titles and artists overlapped. Because of that, we decided to randomly select 20 playlists and show where the different songs that belong to each playlist placed on the map. Each playlist has its own color. This map for the W2V input is depicted in \ref{fig:som_map}. Obviously, the playlists do not really form any visible clusters.
\begin{figure}[h]
    \centering
	\includegraphics[width=140mm]{./img/som_map.png}
	\caption{The location of different songs from 20 randomly selected playlists on the map created by SOM. Each playlist has its own colour.}
	\label{fig:som_map}
\end{figure}
\subsubsection{Results}
The fact that the playlists do not form clusters is also supported by the fact that the overall results for the self organizing map algorithm are quite poor. Actually it is the worse method that we implemented and our hope to improve the results of W2V were not fulfilled. The threshold did not really save it either. The results improved but it still stayed at the bottom of our method ranking.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth { | c || X[c] | X[c] | X[c] | X[c] | X[c] |}
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 SOM with W2V & 0.00103 & 0.00427 & 0.00720 & 0.00200 & 8034 \\
 \hline
 SOM wiht PCA\_Tf-idf & 0.00044 & 0.00208 & 0.00462 & 0.00125 & 8243 \\
 \hline
\end{tabu} \\
\caption{Table summarizing average SOM values averaged over the 5 cross validations}
\label{table:som}
\end{table}
The distribution graph of the SOM\_W2V method clearly shows, that the distribution of ranks is random or worse which is also suggested by the fact that the average rank is 8243 which almost in the middle of 16594. The main reason for the failure of this method is unclear but it is possible that the data from the W2V vectors that already contain too little information\todo{spis ze informace je too compressed} for the SOM network to be able to cluster data based on it.  But since we recieved even worse results for the SOM\_Tf-id as can be seen in Figure \ref{fig:som_tf_idf_distribution} we are more inclined to another possibility which is, that the SOM network is not be able to provide satisfying results because two dimensions are simply too little to represent the whole complexity of the input data.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  	\includegraphics[width=1\linewidth]{./img/som_w2v_graph.png}
	\caption{Distribution of ranks of songs from the test set the SOM method assigned them.}
	\label{fig:som_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/som_tf_idf_graph.png}
  \caption{Distribution of ranks of songs from the $p_{test}$ for tf-idf vectors as input into SOM}
  \label{fig:som_tf_idf_distribution}
\end{minipage}
\end{figure}

\section{Simple audio method experiments}

\subsection{Audio preparation}\label{ssec:audio_prep}

In order to encode audios of songs, we first needed to acquire some form of the music to become our standard. To make our audio information suitable for machine learning, we decided to extract audios of the same duration so our vectors (spectrograms, mel-spectrograms and mfccs are also of the same length). Since all songs have different lengths and also, one complete 3.5 minute long song results in a spectrogram of size 5214x2206 which when flattened is a vector of length 11502084. Therefore we decided to extract 15 second excerpts from each song to create spectrograms, mel-spectrograms and MFCCs from those. We took 5 seconds between the 15th and 20th second, 5 seconds starting in the middle of the song and 5 second starting 15 seconds before the end. We did not start at the beginning and the end because in some songs, there is silence or applause or some talking before the actual song starts. \\

It was also necessary to decide on some parameters for spectrograms, mel-spectrograms and MFCCs. As stated previously, our neural networks were inspired by \cite{inproceedings_RNNs} where they also performed parameter optimisation. We decided to use their parameters as input for all our methods. It is possible that some parameters values suite some methods better than others but because of the amount of methods we tested we decided it would be too time consuming to optimise them for each method. We also think, that it is a good idea to have all methods work with more or less the same input as their comparison is less ambiguous. \\
The resulting choices were following. We set window width $w$ to 0.2 and window overlap $w_o$ to 0.5$w$ = 0.1. For mel-spectrograms it is also necessary to choose Mel frequency bands which was set to 320 as in our inspiration paper findings\todo{zbytecne to opisujes (in our inspiration paper), proste sem zopakuj referenci }, the performance of neural networks did not increase for values higher than 320. For MFCC coefficients we decided to set the number of MFCC coefficients to 320 which is the same as the number of Mel frequency bands.

We used Python's \texttt{librosa} library \cite{brian_mcfee_2019_2564164} to cut our\todo{snaz se pouziti "our" co nejvic zredukovat - zrovna tady potreba neni} songs and to generate our spectrograms, mel spectrograms and MFCCS using functions \texttt{librosa.core.stft} for spectrograms \texttt{librosa.feature.mel\_spectrogram} for mel-spectrograms and \texttt{librosa.feature.mfcc} for MFC coeficients. The audio data for training was stored in .wav files.

\subsection{Raw Mel spectrograms}\label{ssec:raw_mels}
The input for mel spectrograms was a 15 second long audio described in \ref{ssec:audio_prep}. Extracting spectrograms does not require any training. It a is mathematical procedure explained in \ref{ssec:mel_spectrograms_intro}. \\
The mel-spectrograms we got after transforming a 15 second long audio with 320 Mel frequencies bands were matrices of size 408x320 which when flattened turned into vectors of size 130560. This turned out to be too long to implement in our application, however, we still tested this method and got the following results. \\
As can be seen in \ref{table:mel_spec_methods} 3.7\% of songs ended up between top ten predictions, 4.3\% in top 50 and 4.7\% in the top 100. These results are actually better than any other method using mel-spectrograms as input was before applying the threshold, except of the PCA\_mel

\todo[inline]{Je treba (nejen tady) vysvetlit, co ktera metoda znamena - idealne ve chvili kdy je popisujes, tak tam pridej neco jako (denoted as GRU in experimental results). Pokud to nejde - treba u 5715 a 320, tak je rozumne to pridat do popisku, nebo do "evaluation protocol"}

\subsubsection{Results}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth { | c || c | c | c | c | c |}
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 Raw mel spectrograms & 0.03696 & 0.04275 & 0.0473 & 0.03063 & 7604 \\
 \hline
 PCA\_mel\_5715 & 0.05287 & 0.06298 & 0.06765 & 0.04317 & 7803 \\
 \hline
 PCA\_mel\_320 & 0.04716 & 0.05928 & 0.06550 & 0.03989 & 8357 \\
 \hline
 GRU\_mel  & 0.04628 & 0.05715 & 0.06285 & 0.03856 & 7601 \\
 \hline
 LSTM\_mel & 0.03197 & 0.04291 & 0.04978 & 0.02750 & 7776\\
 \hline
\end{tabu} \\
\caption{Table summarizing average rank values for all methods with mel-spectrogram input averaged over the 5 cross validations}
\label{table:mel_spec_methods}
\end{table}
  \todo{u Figure5.6 chybi popisek toho co je ta cervena plocha}
The patterns observed in \ref{fig:mel_graph} which shows the distribution of ranks for raw mel-spectrograms is similar to all other methods we are studying (except of those that appear to be random such as SOM). The short playlists go through a short drop at the beginning. Longer playlists a a bit more stable byt the results for them are not very convincing, especially not for playlists of length 21 and more. 

\begin{figure}[h]
    \centering
	\includegraphics[width=120mm]{./img/mel_graph.png}
	\caption{Distribution of ranks of songs from the test with their similairity defined as cosine distance between mel-spectrograms.}
	\label{fig:mel_graph}
\end{figure}

\subsection{Raw MFCCs}
Our 15 second audios were fed as input into the texttt{librosa.feature.mfcc()} function with parameters mentioned in \ref{ssec:audio_prep}. Training was not necessary since acquiring mel-frequency cepstral coefficient is a matter of Fourier Transformations. The resulting matrix had the shape of 646x128. When flattened we get a vector of length 82866. This has proven to be too long for practical use in our application, however we still have the evaluation results when defining song similarity as the cosine distance between flattened MFCC matrices.

\subsubsection{Results}

The results of MFCCs are quite poor even compared to our other methods. Only 0.4\% of songs missing in our playlists rank betwen the top 10, 0.9\% in the top 50 and 1.4\% in the top 100 as we can see in Table \ref{table:mfcc_methods}. It was better than the LSTM\_MFCC method before we applied the threshold to our evaluation.
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth { | c || X[c] | X[c] | c | X[c] | X[c] |}
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 Raw MFCC & 0.00415 & 0.00919 & 0.01423 & 0.00607 &  7552 \\
 \hline
 LSTM\_MFCC & 0.03887 & 0.04935 & 0.05670 & 0.033058 & 7774 \\
 \hline
 GRU\_MFCC & 0.03769 & 0.04737 & 0.05438 & 0.032151 & 7774 \\
 \hline
\end{tabu} \\

\caption{Table summarizing average rank values for all methods with MFCC input averaged over the 5 cross validations with threshold.}
\label{table:mfcc_methods}
\end{table}

 When looking at our traditional RDG, we again observe a drop for short playlists, which is not as sharp as for other methods. On the other hand, the rankings for longer playlists, does not seem to be very stable so overall, this method does not behave as we would wish.
 
\begin{figure}[h]
    \centering
	\includegraphics[width=120mm]{./img/mfcc_graph.png}
	\caption{Distribution of ranks of songs from the test set the MFCC method assigned them.}
	\label{fig:mfcc_graph}
\end{figure}


\subsection {PCA on spectrograms}

\subsubsection{Input}
We used spectrograms as described in Section \ref{ssec:audio_prep} as input for our PCA. Because the output of the \texttt{librosa.core.stft()} method is a complex matrix, we computed the absolute value of each matrix entry. Afterwards, we flattened the matrix into a single vector and normalized it. Normalization was very important, without it, the resulting rankings were almost random.

\subsubsection{Training}
Training proved to be a little bit challenging with input vectors of length 900048. As the whole (16594x900048) matrix did not fit into the PCA at once. So instead of using \texttt{sklearn.decomposition.PCA} we turned to a PCA with the possibility to be trained in batches. This kind of PCA is also provided by \texttt{skearn} in the \texttt{sklearn.decomposition.incrementalPCA} module. \\ Our batch size was 1106. Again, when we tried to increase it, we got a memory error. This was a little inconvenient as for the PCA the maximum number of components is $min(n\_samples, n\_features)$. In our case, $n\_samples$ was only 1106 components which explained about 57\% of the dataset's variance.
\todo[inline]{Tady jsem moc nepochopil proc zrovna 1106 - asi bude lepe to vysvetlit online}
\subsubsection{Output}
Our output were vectors of length 1106 which we acquired when taking the trained incremental PCA model and calling \texttt{tranform()} with the spectrogram as a parameter.

\subsubsection{Results}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabu} to 1\textwidth { | c || c | c | c | c | c |}
 \hline
 \textbf{method} & \textbf{R@10} & \textbf{R@50} & \textbf{R@100} & \textbf{nGDC} & $ \boldsymbol{\overline{rank}} $ \\
 \hline
 \hline
 PCA\_spec\_1106 & 0.04747 & 0.05915 &  0.06472 & 0.03982 & 7797 \\
 \hline
 PCA\_spec\_320 & 0.03166 & 0.04289 &  0.05094 & 0.02890 & 7496 \\
 \hline
 GRU\_spec\_20400 & 0.04287 & 0.05196 & 0.05723 & 0.03563 & 7824 \\
 \hline
 GRU\_spec\_5712 & 0.00248 & 0.00628 & 0.01076 & 0.003757 & 7761 \\
 \hline
 LSTM\_spec\_20400 & 0.03641 & 0.05096 & 0.05921 & 0.03126 & 7786 \\
 \hline
 LSTM\_spec\_5712 & 0.01892 & 0.03263 & 0.04252 &  0.01899 & 7643 \\
 \hline
\end{tabu} \\
\caption{Table summarizing average rank values for all methods with spectrogram input averaged over the 5 cross validations}
\label{table:spec_methods}
\end{table}
 

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/pca_spec_1106_graph.png}
  \captionof{Distribution of ranks of songs from the $p_{test}$ set the spectrograms method assigned them.}
  \label{fig:pca_spec_1106_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/pca_spec_320_graph.png}
  \captionof{Distribution of ranks of songs from the $p_{test}$ for spectrograms vectors as input into PCA}
  \label{fig:pca_spec_320_distribution}
\end{minipage}
\end{figure}

The dimensionality reduction in this method was the biggest of all or our methods. Our song representation went from 900048 to 1106 dimensions. It seems though that even with only 57\% of explained variance this method's results were very good compared to other methods. \\
When seeing this, we decided to go for a even more radical reduction to vectors of length 320. This was inspired by the number of mel-filters that were applied to create the mel-spectrogram. We called our bigger model PCA\_spec\_1106 and our smaller model PCA\_spec\_320.  The results of the smaller model were only slightly worse than those of the big one with another big dimension reduction. One interesting thing is, that the PCA\_spec\_320 method is the only one whose results worsened with the appliacation of the threshold. It can be observed in Figure \ref{fig:method_comparison} where the PCA\_mel\_320 is depicted in yellow. The impairment is only slight for all playlists but when looking at the graph on the right with results for long playlists, the deterioration is quite significant. \\
\todo[inline]{Neni nahodou dimenze W2V 300? Pak nechapu proc zrovna 320?}

\subsection{PCA with Mel-spectrograms}
\subsubsection{Input}
The input for our Mel-PCA were mel-spectrograms from \ref{ssec:audio_prep} which were flattened and normalized using a \texttt{MinMaxScaler} from the \texttt{sklearn} Python package.

\subsubsection{Training}
Unlike with the spectrograms, mel-spectrograms fit into the memory so the PCA was able to train with the whole dataset at once. This allowed us again to find out what number of components explaines 90\% of the variance ratio and to use it. We found that 90\% of variance is explained by 5715 components which is also what we used to train our first model. \\
We were quite pleased by the results as it is the best method that we came up with. Therefore we were quite hopeful when reducing the dimension even more to 320 as we did with PCA on spectrograms. We again created two models, one is called the PCA\_mel\_5715 the other the PCA\_mel\_320. 

\subsubsection{Output}
Our output for the bigger model was a vector of length 5715, for the smaller model, it was a vector of length 320.

\subsubsection{Results}
To our disappointment, the more radical dimension reduction did not improve the performance of the PCA\_mel method. It actually worsened the results significantly and the PCA\_mel\_320 was our worse method. But only until we applied the threshold. For this method, the threshold worked like magic. Its results improved over ten times. It is interesting because it was the PCA\_spec\_320 for which the results worsened after applying the threshold and one might think, that PCA methods on audio will behave similarly.
\todo[inline]{No, spojeni "like magic" bych se radeji vyhnul... Vzhledem k tomu, co vsechno bylo povazovano za "magic" predtim nez to veda vysvetlila bychom mohli byt obvineni z toho, ze jsme se nepidili dostatecne po duvodech toho zlepseni:-)) Jinak celkova rada: vedec je od toho, aby vysledky zaznamenal a pouzil - to ze mu udelaly radost, nebo ho zklamaly je irelevantni (ne ze by to tak nebylo, ale do clanku se to tak nepise:-)). No a potom tu chybi tabulka jako u predchozich kapitol... }

Figures \ref{fig:pca_mel_5715_distribution} and \ref{fig:pca_mel_320_distribution} illustrate the distributions for PCA\_mel\_5715 components and PCA\_mel\_320. Another thing to notice in the Mel\_PCA\_{5715} graph is that the number of the top 1-5 rankings for longer playlists is not quite as low compared to all the other methods we tested. We still observe the trend of cosiderably higher number of the top 1-5 for shorter playlists, however, it is not as significant compared to for example the the method with raw Mel-spectrograms.
\begin{figure}[h]
\centering
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/pca_mel_5715_graph.png}
  \caption{Distribution of ranks of songs from the $p_{test}$ set the spectrogram method assigned them.}
  \label{fig:pca_mel_5715_distribution}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/pca_mel_320_graph.png}
  \caption{Distribution of ranks of songs from the $p_{test}$ for spectrogram vectors as input into PCA}
  \label{fig:pca_mel_320_distribution}
\end{minipage}
\end{figure}\label{fig:pca_mel_comparison_graps}

\section{Deep audio experiments}

\subsubsection{Architecture}
\todo[inline]{Tady konecne nachazim to, co mi chybelo v kapitole 4 (a mozna i 3) - podle me by architektura mela byt uz jejich soucasti a v experimentech by ses mela zabyvat uz jenom tim jak jsi to vyhodnocovala a jak to dopadlo - zvaz pripadne presuny..}
Before analysing each or the audio method using deep neural networks independently we will describe the two architectures we used. \\
As stated multiple times before, we decided to build our network based on the \cite{inproceedings_RNNs} paper. We chose our audio extraction coefficient based on their findings and we also designed our architecture in a way they did with some slight adjustments and extensions. \\
The first most notable thing is, that their network was designed to classify sounds, not encode songs. One might think that this would make their RNN unsuitable for our task, and it did as a whole. However their network consisted of two parts. The first part was an autoencoder and the second part a multi-layer perceptron. The autoencoder was trained in an unsupervised manner and the outputs were then fed into the MLP which did the classification. \\
For the purposes of our work, we only used the autoencoder part. Unlike them, instead of using the \texttt{auDeep} library we decided to build our networks with the \texttt{Keras} library \cite{chollet2015keras} as it has a user-friendly model-creation API for Python. We had also access to GPU computers and \texttt{Keras} (in our case with the \texttt{Tensorflow} backend) makes it easy to train networks faster with the access to GPUs. \\
We created two architectures. One with two GRU layers at the beginning as the encoder and one Bidirectional layer as the decoder. This follows the paper. We also decided to create another architecture with LSTM layers instead of GRU layers even though \citeauthor{inproceedings_RNNs} found in their work that the additional complexity did not yield better results. Both architectures can be seen in Figure \ref{fig:nn_architectures} \\

\begin{figure}[h]
\centering
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/gru_architecture.png}
  \captionof{The general architecture of our GRU neural networks}
  \label{fig:gru_architecture}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/lstm_architecture.png}
  \captionof{The general architecture of our LSTM neural networks}
  \label{fig:lstm_architecture}
\end{minipage}
\end{figure}\label{fig:nn_architectures}


The main motivation behind using LSTMs as well as GRU layers in our thesis was that LSTM layers are specifically suited for sequential data such as audio. GRU layers are a simplified version of LSTMs. We were hoping that the more complex layers could encode our audio data into the same dimensions as the GRU networks but help to make the similarities more accurate. \\

We used the \textit{mean squared error} as our loss function which is the standard for autoencoder networks. Our optimiser was \textit{adam} the same as in the \cite{inproceedings_RNNs} paper. We had to decrease the learning rate from 0.001 to 0.0001. Before we did that, our resulting predicted vectors often consisted of just \textit{NaN}s.

\subsubsection{Inputs and outputs}
Both the GRU and the LSTM network had three kinds of inputs --- the spectrograms, mel-spectrograms and the MFCCs. They were all passed in the form of 2D matrices containing $(n\_features, n\_timestamps)$. GRU as well as LSTM networks take 2D matrices and not just 1D vectors as input. Before training, the inputs were normalized using the \texttt{MinMaxScaler} with values between 0 and 1. \\
One important thing to note here is that these kind of networks that process sequential data in the form of 2D matrices reduce the dimensionality of only the $ n\_features $ and not the $ n\_timestamps $. Our 15 second audios yielded 408 time stamps and 2206 features for spectrograms and 408 time stamps and 320 features for mel-spectrograms. With MFCCs the number of time stamps was 646 and the number of features 128. Therefore we did not attempt a dimensionality reduction as big as with PCA which does not care if a feature is a time stamp or a sample.

\subsection{GRU network with spectrogram input}

\subsubsection{Training}

We trained two GRU spectrogram networks with variable output vector lengths. We decided to base our vector length on the PCA's explained variance ratio. For spectrograms however, we only found out that 1106 explains 56\%of variance. Therefore, we took the information from the mel-spectrogram PCA where 5715 explains 90\% of variance and did a simple calculation $$ l(mel\_spec_{transformed})/l(mel\_spec) = l(spec_{transformed})/l(spec) $$. To keep the proportional reduction same as for mel-spectrograms and the output length of 5715 where $l(x)$ is the length of either the spectrogram or mel-spectrogram flattened. This would mean an output vector length of almost 40000. We decided that it would be too much for any practical use in our web application and reduced it to 20400 which at that point we thought could be potentially used. The shorter vectors used as encodings of our spectrograms were of length 5712. This inspired by the Mel-spectrogram PCA as we thought that the neural network could mimic the Mel-frequency reduction as well as additional reduction. \\
In our base paper, they trained their autoencoders for 50 epochs using batch sizes of 64. We found this to be insufficient especially with our learning rate reduction. For GRU networks with spectrograms we set the number of epochs to 100 and the batch size to 295 (for higher batch values we got memory errors). 

\subsubsection{Results}
As can be observed in Figure \ref{fig:all_model_training} the loss is basically constant for both GRU networks with spectrogram input (the GRU\_spec\_20400 is hidden behind the orange line of GRU\_spec\_5712 as their progress or rather the lack of it is the same). This means, that the network did not really improve for some reason. Therefore it is not suprising that the GRU\_spec\_5712 is not between our best ones. And it is quite surprising, that the GRU\_spec\_20400 even though it appeared to be quite bad before the application of the threshold, it greatly improved and became the best method with spectrogram input as can be seen in Table \ref{table:spec_methods}. The table also shows that 4.3\% of songs was ranked in the top 10, 5.2\% in the top 50 and 5.7\% in the top 100 for the longer GRU spectrogram model. For the short GRU spectrogram model, the results are significantly worse with 0.2\% for the top ten songs 0.6\% ranked in the top 50 and a little over 1\% ranked in the top 100. \\
The distributions depicted in Figures \ref{fig:gru_spec_20400_distribution} and \ref{fig:gru_spec_5712_distribution} of both GRU\_specs is are dissimilar too. The overall trend for the short playlist drop and stability of long playlists is visible in \ref{fig:gru_spec_20400_distribution} but not in the \ref{fig:gru_spec_5712_distribution} where there is a trace of this trend but the results are overall bad so it fades. 

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/gru_spec_20400_graph.png}
  \caption{The distribution of predicted ranks using the long GRU spec encodings.}
  \label{fig:gru_spec_20400_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/gru_spec_5712_graph.png}
  \caption{The distribution of predicted ranks using the short GRU spec encodings.}
  \label{fig:gru_spec_5712_distribution}
\end{minipage}
\end{figure}\label{fig:gru_spec_distributions}

\subsection{LSTM network with spectrogram input}

\subsubsection{Training}
We chose the same training strategy for LSTMs with spectrogram input as we did for the GRU\_spec networks. It makes the comparison of both methods more fair. We created two versions of our LSTM models, one LSMT\_spec\_20400 and the shorter version LSTM\_spec\_5712. The lengths are also the same as with GRU\_specs and the motivation behind them is identical.

\subsubsection{Results}
Unlike the GRU model training which showed almost no improvement even after 100 epochs of training, our LSTM\_specs as one can observe in Figure \ref{fig:all_model_training} where the LSTM\_spec\_20400 is rendered in green and the LSTM\_spec\_5712 in red went through at least some kind of progress. The decrease of loss slowed down significantly towards the end of the 100 epochs. \\
The higher training improvement correlated with better results for the LSTM\_spec methods. But only until we applied the threshold. It raised performance of all methods but the increase for GRU\_spec\_20400 was so big that it vaulted over both LSTM methods. In Table \ref{table:spec_methods} are the results of both LSTM\_spec models. \\
A thing worth noting is that with the bigger dimension reduction, the results worsened even before applying the threshold. And the LSTM\_spec\_20400 method stayed better even after the threshold evaluation as we can see in Table \ref{table:spec_methods} and in Figures \ref{fig:lstm_spec_20400_distribution} and \ref{fig:lstm_spec_5712_distribution} which visualize the difference.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/lstm_spec_20400_graph.png}
  \caption{The distribution of predicted ranks using the long GRU spec encodings.}
  \label{fig:lstm_spec_20400_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/lstm_spec_5712_graph.png}
  \caption{The distribution of predicted ranks using the short GRU spec encodings.}
  \label{fig:lstm_spec_5712_distribution}
\end{minipage}
\end{figure}\label{fig:lstm_spec_distributions}

\subsection{GRU and LSTM networks with Mel-spectrogram input}

\subsubsection{Training}
The GRU\_mel and LSTM\_mel networks were both trained under the same conditions. We trained them on our mel-spectrograms for 150 epochs with a batch size of 256. With a higher batch size we got a memory error. The output length of the encoded song vector was based on variance ratio of the PCA\_mel (same as for networks with neural networsk with spectrogram input). We did not attempt any further reductions here partly because as stated before, only the features can be reduced by GRU and LSTM layers meaning, that we have to have vectors of length of at least 408 and to scale the features down to one or two might lead to same results as with our SOM methods.

\subsubsection{Results}
Mel-spectrograms seem to be the best kind of input we have and the GRU\_mel network confirms it as it has the best results within the neural network method group. The GRU\_mel network performed better than the one with LSTM layers and also showed the smallest loss in training. The difference in ranking accuracy is quite big. Figures \ref{fig:gru_mel_distribution} and \ref{fig:lstm_mel_distribution} display typical tendencies all most of our methods have, with the sharp drop for short playlists and more stability for longer playlists. This is especially apparent with the GRU\_mel method. For the LSTM\_mel, longer playlists also drop quite early. \\
Table \ref{table:mel_spec_methods} puts recalls and nDGC of our "mel" neural networks into perspective with other "mel" methods.
As we can see, the GRU\_mel network placed 4.6\% of songs into the top 10, 5.7\% into the top 50 and 6.3\% into the top 100. The LSTM\_mel network is worse as and is places as our worst "mel" method for \textit{R@10} with 3.2\% of songs in the top 10. For \textit{R@50} and \textit{R@100} it outperforms raw mel-spectrograms with 4.3\% of songs in the top 50 and 5\% of songs with assigned ranks in the top 100.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/gru_mel_graph.png}
  \caption{An RDG of the GRU\_mel method}
  \label{fig:gru_mel_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/lstm_mel_graph.png}
  \caption{An RDG of the LSTM\_mel method}
  \label{fig:lstm_mel_distribution}
\end{minipage}
\end{figure}\label{fig:mel_nn_distributions}

\subsection{GRU and LSTM networks with MFCC input}

\subsubsection{Training}
At first we did not plan on using MFCCs as input into neural networks. However because it turned out the the raw MFCCs are too long to be used in our application directly we decided it would be a shame to just leave them out and tried to reduce their dimension again with both our GRU and LSTM network architectures. \\ 
We trained both architectures with 150 epochs and a batch size of 256 which is the same as the neural networks having mel-spectrograms as input. 

\subsubsection{Results}

The MFCC networks seem to have the biggest potential of improving if they were to be trained for a longer period of time as their plots in Figure \ref{fig:all_model_training} especially the GRU\_MFCC one does not stagnate as much as the other networks. They would however probably never decrease their loss to be as small as for the "mel" networks. \\
Compared to the raw MFCCs the GRU\_MFCC and LSTM\_MFCC meant an improvement as the values in \ref{table:mfcc_methods} suggest. The Figure \ref{fig:mfcc_nn_distributions} containing both method's \textit{RDGs} confirms the superiority of the GRU\_MFCC method over the LSTM\_MFCC. The LSTM\_MFCC and especially the GRU\_MFCC method made a huge jump upwards with the threshold similarity. Without it, they seemed to have really bad results, but the threshold helped them to perform quite well.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/gru_mfcc_graph.png}
  \caption{The RDG of GRU\_MFCC}
  \label{fig:gru_mfcc_distribution}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{./img/lstm_mfcc_graph.png}
  \caption{The RDG of LSMT\_MFCC}
  \label{fig:lstm_mfcc_distribution}
\end{minipage}
\end{figure}\label{fig:mfcc_nn_distributions}



\section{Discussion}\label{sec:discussion}

All the experiments we performed yielded a lot of data, to be fair more than we planned. In this discussion we try to compare our findings with the expectations we had and interpret their similarities and differences. Overall we have to admit that the results we obtained were a bit worse than we thought they were going to be, especially for the audio methods. We will try to explain the reasons. \\
The fact that the threshold evaluation improved our methods so much suggests that the users have groups of songs in their playlists that are very similar but the groups can be quite dissimilar and that songs, that are somewhat similar to all groups "pollute" the recommendations. \\
We tested the playlists on diversity. When we divided the number of unique artists with the playlist's length and averaged over all the playlists we got 0.83 which means, that an artist rarely repeats in a playlist. This might be one reason why our methods are not as good, because the playlists are just strange.\todo{radeji jiny vyraz} To make such an excuse kind of contradicts our intentions to introduce the user to new songs and especially be good in recommending songs to users with an unusual music taste. Because a dataset with playlists from users with strange music tastes would be like our dream dataset. \todo[inline]{Jako tusim cos tim chtela rict, ale to co je napsane mi moc smysl nedava - jeste na to mrkni, pripadne k online diskuzi}\\

To summarize the results and put all the methods into perspective we did two things. First we created a ranking table, where we chose some measures of our methods and ranked them on how well they performed in that particular measure. This was to compare the methods with respect to each other. Secondly we focused on the overall values and how good they are. This was to estimate the recommendation qualities of our methods.\\
Additionaly we also created some more interesting graphs. We tried to find a correlation between the training loss of neural networks and their performance. Also between the 
What we just presented are pure numbers so also show something more practical, we picked a song at random and then 
\begin{figure}[h]
    \centering
	\includegraphics[width=120mm]{./img/all_training_graphs.png}
	\caption{The training mean squared error loss values for all the final neural networks we trained.}
	\label{fig:all_model_training}
\end{figure}

Even with the threshold improvement, our results are still worse than we were hoping for. 

