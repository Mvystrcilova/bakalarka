\chapter{Audio-based methods}
In this section we will describe the possibilities of how to transform an audio file (in our case .wav) into a mathematically and thus computer understandable model that will also be relevant for music similarity measurement. This process consists of many steps and a lot of research has been done on all of them. \\
First we have to extract the signal and get a description of its features. The second step is to feed these descriptions into some other either simple machine learning or deep learning algorithm to get a vector representation. This vector then will be used to compute similarity between songs. \\

\section{Related work}
For the signal representation, there is obviously the possibility to use raw audio data as input for the machine learning algorithm. This however is a quite uncommon approach and when tested recently in tag prediction \cite{6854950} it had worse results than the standard spectrogram. Multiple studies and experiments have been done using spectrograms as representation for classification tasks as for example \cite{wang2014improving} or to for unsupervised learning, which is what also our area of interest \cite{van2013deep}, \cite{Ramakrishnan2017song2V}, \cite{NIPS2009_3674}. \\
In these studies the inputs were fed into various neural network. The output of those was then evaluated and compared to trying to classify music or simply estimate music similarity with using similarity measures directly on spectrograms. We are going to do the same thing in this thesis.
\section{Basic audio representation methods}
Sound is as vibration that spread through gas, liquid or solid as a wave of pressure. For humans, the sound we hear is has a frequency between 20Hz and 20kHz. Other sound waves are inaudible for humans. \\

\subsection{Raw waveform}
The most basic representation of an audio signal is a \textit{waveform}. It captures the variation of pressure over time. As we cannot store infinite data to capture the state of the wave in every moment, we need to establish a sample rate. The sample rate is the number of samples per second at which the pressure is recorded as amplitude. Common sample rates are 44 100 Hz and 22 050 Hz that capture oscillation up to 22 050 Hz and 11 025Hz.
\subsection{Spectrograms}
Raw waveform data have a lot of data points which make them spaciously demanding. Luckily, they also display strong regularities in their oscillations. Therefore, we have another, more compact possibility to represent audio signal. It encodes the signal as the strength of oscillations at different frequencies as opposed to amplitudes over time. Such an encoding is called a \textit{spectrum} when the sinusoids are used as prototypical oscillations.
The spectrum is obtained from the waveform by applying Discrete Fourier Transformation. The signal is then represented by oscillations of a few frequencies spanning the full signal. \\
However a problem with this approach is, that for longer recordings, many oscillations are present only over some limited time span or change frequency. To represent all the oscillations the \textbf{Short-Time Fourier Transformation} can be computed. It slices the audio into small often overlapping windows, computes their spectra and then puts them together in a chronological order. The This spectra matrix is called the \textit{spectrogram}. It can be visualized as a graph with frequency on one and time on the other axis. The intensity of a frequency is represented by color.

\begin{figure}[h]
    \centering
	\includegraphics[width=140mm]{./img/ilustrative_spectrogram.png}
	\caption{Spectrogram of the song 'Someone Like You' by 'Adele'. The intensity of different frequencies over time is converted to decibels.}
	\label{fig:ilustrative_specrogram}
\end{figure}

\subsection{PCA}

PCA is a common machine learning algorithm to reduce dimensionality of the feature space. It tries to keep only features with most variance and discards feature in which all the data points are highly correlated. The data space is transformed in such a way, that the first principal component (PC) has the largest possible variance, the second PC the second largest variance etc. \\
Mathematically it is a orthogonal linear transformation to a new coordinate system where the base vectors are the principal components. To achieve this, we first need to center the data around the origin. That is done by subtracting the mean of each variable from the data. After that a covariance matrix is computed with its eigenvalues and corresponding eigenvectors. After normalizing the eigenvectors, they can be interpreted as a new basis vector. This new basis transforms the covariance matrix so that it becomes diagonal. Each of the diagonal elements represents the variance of each axis. All the components without any reduction give us the whole information about the input data, only in a vector space with a different basis. Every component explains some portion of the data's variance. The variance can be calculated by dividing the eigenvalue corresponding to an eigenvector with the sum of all eigenvalues. The dimensionality reduction is dependant on how many components we want. If we want to visualize the input data in a 2D graph, we only have a space with the first two PCs. The reason to use this algorithm in our thesis would be mainly to reduce the length of the audio feature vectors. \\
The PCA assumes that there is linear correlation between features. If there is not, the PCA will not discover it and will loose a lot of information with the dimensionality reduction it performs.

\subsection{Mel Spectrogram}\label{ssec:mel_spectrograms_intro}
Mel-spectrograms are another approach to reducing the dimensionality of our data. However they are not mathematically based reductions of any kind of feature space. They are filtered spectrograms.Frequency bands are extracted by applying triangular \textit{Mel scale} filters to the power spectrum. The \textit{mel scale} after which this the spectrograms are called was named in 1937 in a study \cite{1937ASAJ....8..185S} by Volkmann and Newman. Since then it has been re-formulated multiple times, to mention for example \cite{mel_scale_fit} by Umesh at al. It is based on the human perception of pitch and loudness and allows us to convert from Hz to Mels which are more discriminatory at lower frequencies and less at higher frequencies - as is the human ear. As for the triangular filters, each of them has a response going from 1 to 0. They respond 1 at the center of some frequency and then their response decreases linearly to 0 towards to the place where they meet the neighbouring filters. When these filters are applied to a spectrogram, we get a mel-spectrogram.

\todo{Obrazek mel scalu}. 

\subsection{Mel Frequency Ceptral Coefficients}
Mel Frequency Ceptral Coefficients (MFCC) are another step further in extracting audio features. They are obtained by applying \textit{Discrete cosine transform} to mel-informed spectrograms. The DCT yield even more compressed representation of them.

\section{Deep audio representation methods}

\subsection{Convolutional neural networks}
Convolutional neural networks are neural networks that have become extensively researched after AlexNet (a form of CNN) was demonstrated in 2012 and outperformed all other methods for visual classification \cite{Krizhevsky:2012:ICD:2999134.2999257}. As with other neural networks, CNN's biggest advantage is to emulate behavior of unknown non-linear functions. CNNs have the ability to map high-dimensional data into a space of finite categories (with hunderds or thousands of classes). They are mostly used in visual imagery tasks. \\
The idea behind convolutional neural networks is to use \textit{local filters} instead of creating fully connected layers. This has risen from the idea that in images, there are correlated compositions on short scale distances, rather than at large distances. For example when detecting a human face in an image with a tree in the background, the tree does not have much to say about the face, unlike the eyes or the nose by which the face can be identified and in a much greater proximity to each other. This is also the reason why in the CNN's architecture, the layers are not fully connected. \\
There are 4 main components that are generally be included in every CNN network. The \textit{Convolution layer},the \textit{ReLU}, the \textit{Maxpooling layer} and the \textit{Fully connected layer} that yields the output. To briefly describe these layers lets start with convolution. The convolutional layer helps to reduce the number of connections and weights. It consists of filters that can be learned. These only take a small number of nearby features into account at a time but extend through the whole input. Each of the filters crates a 2D activation map by computing the dot product of entries of the filter and the input. ReLU (rectified linear unit) is generally used to increase non linear properties of the decision function. It's function is $ f(x) = max(0.x) $ which is applied to the results of the convolution to speed up training without affecting the receptive fields of the convolution layer. The pooling layer also reduces the number of parameters and helps prevent over-fitting. The most common function to implement is \textit{max pooling}. The features are partitioned into a set of non-overlapping rectangles (if input is 2D) and each of these rectangles  is represented by its maximum value. 
The final layer is usually a fully connected. Its neurons have connections to all activations of the previous layer and their activation is then computed as an affine transformation. 

\subsection{Deep belief networks}
Deep belief networks are multiple \textit{Restricted Boltzmann machines} stacked on top of each other. RBMs are shallow two-layer neural nets. The first layes is called the visible layer, the second layer is called the hidden layer. The nodes of each layer communicate with with the previous and subsequent layer but there are no connections between nodes of the same layer. Each visible node takes a low level feature to be learned and multiplies it by some weight. The results for each feature of an input sample are then summed, bias is added and this is the result passed through and activation algorithm which produces an output for each hidden node. These outputs then can be redirected into another hidden layer instead of the output of the neural network. \\
RBMs also have the ability to reconstruct data without supervision. When the input makes it through both layers it then becomes input for the hidden layer and travels through the neural network in the opposite direction. The activations are multiplied by the same weights and passed to the visible layer where a new bias is added. The output of the visible layer is then compared to the initial input and the network adjusts weights so that it minimizes the difference between the input and the output.

\subsection{Recurrent neural networks}
Recurrent neural networks have one major difference compared to other neural networks. They include feedback loops in their structure which allows them to exhibit dynamic behaviour and makes them useful for processing sequential data. RNNs have a hidden state that is determined by previous states and is updated with every subsequent step. There are many variants of RNNs such as \textit{Fully recurrent, Independent RNNs, Long short-term memory, Gated recurent units, Bi-directional} or \textit{Recurrent multilayer perceptron networks}. 

\begin{itemize}
    \item \textbf{Fully recurent RNNs} simple RNNs. They are build as multi-layer perceptrons with hidden unit activations that send feedback into the network along with inputs.
\end{itemize}

\section{Conclusion and implementation choices}

\subsection{Basic audio representation choices}
At first when developing the web application we had the hope of using at least some of the raw data representation, to build some sort of base-line model. However, this turned out to be quite difficult as the vectors from spectrograms, mel-spectrograms and mfccs have tens sometimes even hundreds of thousands of features. Therefore, altghough we implemented and evaluated recommendations based on mel spectrograms and mfccs, to implement them turned out to be too time consuming (for a running application). The specific reasons are described in more detail in the \textbf{Experiments chapter}. For the running application, only PCA transformed spectrograms and mel-spectrograms were selected as they allow us to significantly reduce the dimensions of vectors. The MFCCs were ommited as it was first thought they will be used as raw MFCCs. Then when this proved to be impossible, they were used as input for a neural network.

\subsection{Deep Audio representation choices}
Neural networks are a quickly developing and expanding field with many various applications. It is difficult to build an accurate neural network for specific task even with experience in deep machine learning. Because that is not what we have, we chose our neural network architecture and parameters based on literature relevant to the topic of audio-based music recommendation. The main requirement we had for our algorithms was that they have to work in an unsupervised manner and reduce the feature space. This reduced the number of possibilities for us considerably, as most architectures are build for music classification (mostly into genres) or speech and sound recognition. We decided that autoencoders seem to suite best to our task in this thesis. They are based on the idea that the autoencoder neural network has two parts the encoder and the decoder. Because we want an unsupervised algorithm, its task is to encode the input sample into a smaller dimension in the encoder part and then decode it again into the same vector with the decoder part. Since we are working with sound data, our choice was to use RNN layers which have the ability to encode sequences (our spectrograms, mel-spectrograms and mfccs are 2D matrices). These choices were mainly inspired by this paper \cite{inproceedings_RNNs}. However since not only here but throughout the whole thesis, we prefer quantity over quality, we implemented not only neural networks with GRU layers but also LSTM layers and used spectrograms, mel-spectrograms and also mfccs as input rather than tuning one particular neural network to give the best results possible. 