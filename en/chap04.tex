\chapter{Audio-based methods}\label{chap:audio_methods}
In this section we will describe the possibilities of how to transform an audio signal (in our case from a .wav file) into representations suitable for song similarity calculations. This process consists of many steps and a lot of research has been done on all of them as illustrated in Section \ref{sec:audio_related_work}. 

The reason we are focusing on audio in this thesis is the notion, that what people care about in a song is its sound. There are patterns in music that are pleasant to the human auditory system, otherwise, music would not be so popular. We believe it is the sound wave that contains these patterns. It is difficult to define what exactly they are, so we hope that with the use of unsupervised machine learning algorithms, we will be able to find them and then locate them in unseen songs as well. 

Figure \ref{fig:audio_extraction} illustrates the steps of audio extraction. The blue part of the diagram describes the steps that are taken to acquire various basic music representations which are explained in Section \ref{sec:basic_music_representation_methods}. Each of these representations can be given as input to a machine learning algorithm as depicted in green from Section \ref{sec:audio_machine_learning} or deep learning algorithm from Section \ref{sec:audio_deep_learning} depicted in purple. Both simple and deep learning algorithms yield a final vector representation of the song. 

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/audio_feature_extraction_steps.png}
	\caption{A diagram displaying the steps taken in audio extraction. ML stands for machine learning and DL for deep learning.}
	\label{fig:audio_extraction}
\end{figure}

\section{Basic audio representation methods}\label{sec:basic_music_representation_methods}
 

\subsection{Raw waveform}
Sound is as vibration that spreads through gas, liquid or solid as a wave of pressure. For humans, the sound we hear has a frequency between 20Hz and 20kHz. Other sound waves are inaudible for humans. The most basic representation of sound as an audio signal is a \textit{waveform}. It captures the variation of pressure over time. As we cannot store infinite data to capture the state of the wave in every moment, we need to establish a \textit{sample rate}. A sample rate is the number of samples per second at which the pressure is recorded as amplitude. Common sample rates are 44,100 Hz and 22,050 Hz that capture oscillation up to 22,050 Hz and 11,025Hz \cite{Schluter2017}.

\subsection{Spectrograms}
Raw waveform data have a lot of data points which make them spaciously demanding. Luckily, they also display strong regularities in their oscillations which gives us a different, more compact possibility to represent audio signal. The signal can be encoded as the strength of oscillations at various frequencies as opposed to amplitudes over time. Such an encoding is called a \textit{spectrum} when sinusoids are used as prototypical oscillations.

The spectrum is obtained from a waveform by applying \textit{Discrete Fourier Transformation}. The signal after DFT is represented by oscillations of a few frequencies spanning the full signal. 

However a problem with this approach is, that for longer recordings, many oscillations are present only over some limited time span or they change frequency. To represent all the oscillations the \textit{Short-Time Fourier Transformation} can be computed. It slices the audio into small often overlapping windows, computes their spectra and then puts them together in a chronological order. This spectra matrix is called the \textit{spectrogram} and for the song 'Someone Like You' by 'Adele' it has the shape of (2206 x 7796). It can be visualized as a graph with frequency on one axis and time on the other axis. The intensity of a frequency is represented by color.

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/spectrogram.png}
	\caption{Spectrogram of the song 'Someone Like You' by 'Adele'. The intensity of different frequencies over time is converted to decibels.}
	\label{fig:ilustrative_specrogram}
\end{figure}


\subsection{Mel Spectrograms}\label{ssec:mel_spectrograms_intro}
Mel-spectrograms are another approach to reducing the dimensionality of the audio data. They are filtered spectrograms. Frequency bands are extracted by applying triangular \textit{Mel-scale} filters to the power spectrum. The \textit{mel scale} after which these spectrograms are called was named in 1937 in a study by Volkmann and Newman \cite{1937ASAJ....8..185S}. Since then it has been re-formulated multiple times, for example by Umesh at al \cite{mel_scale_fit}. It is based on the human perception of pitch and loudness and allows us to convert from Hz to Mels. Mels are more discriminatory at lower frequencies and less at higher frequencies - as is the human ear. 

Each of the triangular filters has a response going from 1 to 0. They respond 1 at the center of some frequency and then their response decreases linearly to 0 towards to the place where they meet the neighbouring filters. When these filters are applied to a spectrogram, we get a mel-spectrogram. It is again a matrix of a smaller shape this time (320 x 7796) for the song 'Someone Like You' and it can also be visualized as illustrated in Figure \ref{fig:ilustrative_melspecrogram}.

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/melspectrogram.png}
	\caption{Mel-spectrogram of the song 'Someone Like You' by 'Adele'. The intensity of different frequencies over time is converted to decibels.}
	\label{fig:ilustrative_melspecrogram}
\end{figure}

\subsection{Mel Frequency Ceptral Coefficients}\label{ssec:mfcc_intro}
Mel-Frequency Ceptral Coefficients (MFCC) are another step further in compressing audio features. They are obtained by applying \textit{Discrete cosine transformation} to mel-spectrograms. For the song 'Someone Like You' by 'Adele' which we used as an example for spectrograms as well as mel-spectrograms, it created a matrix of shape (128 x 7796) which looks as Figure \ref{fig:ilustrative_mfccs} illustrates. 

\begin{figure}[h!]
    \centering
	\includegraphics[width=140mm]{./img/mfccs.png}
	\caption{MFCCs of the song 'Someone Like You' by 'Adele'. The intensity of different frequencies over time is converted to decibels.}
	\label{fig:ilustrative_mfccs}
\end{figure}

A nice introcution to music signal processing with respect to deep machine learning, where spectrograms, mel-spectrograms and MFCCs are explained in more detail can be found here \cite{Schluter2017} where we also drew a lot of our information from.

\section{Simple audio representation methods}\label{sec:audio_machine_learning}

\subsection{PCA}
PCA is a common machine learning algorithm used to reduce dimensionality of the feature space that was invented by Karl Pearson in 1901 \cite{doi:10.1080/14786440109462720}. It tries to keep features with most variance and discards feature in which all the data points are highly correlated. The data space is transformed in such a way, that the first principal component (PC) has the largest possible variance, the second PC the second largest variance etc. 

Mathematically it is a orthogonal linear transformation to a new coordinate system where the base vectors are the principal components. To achieve this, it is first necessary to center the data around the origin. That is done by subtracting the mean of each variable from the data. After that a covariance matrix is computed with its eigenvalues and corresponding eigenvectors. After normalizing the eigenvectors, they can be interpreted as the new basis vector. This new basis transforms the covariance matrix so that it becomes diagonal. Each of the diagonal elements represents the variance of each axis. All the components without any reduction give us the whole information about the input data, only in a vector space with a different basis. Every component explains some portion of the data's variance. 

The variance can be calculated by dividing every eigenvalue corresponding to an eigenvector with the sum of all eigenvalues. The dimensionality reduction is dependant on how many components we want. If we want to visualize the input data in a 2D graph, we create a space with only the first two PCs as a base and map all the data onto it. The reason to use this algorithm in our thesis is be mainly to reduce the length of the audio flattened audio matrices. 

The PCA assumes that there is linear correlation between features. If there is not, the PCA will not discover it and will loose a lot of information with the dimensionality reduction it performs.

\section{Deep audio representation methods}\label{sec:audio_deep_learning}

\subsection{Convolutional neural networks}
Convolutional neural networks are neural networks that have become extensively researched after AlexNet (a form of CNN) was demonstrated in 2012 and outperformed all other methods for visual classification \cite{Krizhevsky:2012:ICD:2999134.2999257}. As with other neural networks, CNN's biggest advantage is to emulate behavior of unknown non-linear functions. CNNs have the ability to map high-dimensional data into a space of finite categories (with hundreds or thousands of classes). They are widely used in visual imagery tasks. 

The idea behind convolutional neural networks is to use \textit{local filters} instead of creating fully connected layers. This has risen from the idea that in images, there are correlated compositions on short scale distances, rather than at large distances. For example when detecting a human face in an image with a tree in the background, the tree does not have much to say about the face, unlike the eyes or the nose by which the face can be identified and are in a much greater proximity to each other. This is also the reason why in the CNN's architecture, the layers are not fully connected. 

There are 4 main components that are generally be included in every CNN network. The \textit{Convolution layer},the \textit{ReLU}, the \textit{Maxpooling layer} and the \textit{Fully connected layer} that yields the output. To briefly describe these layers lets start with convolution. The convolutional layer helps to reduce the number of connections and weights. It consists of filters that can be learned. These only take a small number of nearby features into account at a time but extend through the whole input. Each of the filters creates a 2D activation map by computing the dot product of entries of the filter and the input. ReLU (rectified linear unit) is generally used to increase non linear properties of the decision function. Its function is $ f(x) = max(0,x) $ which is applied to the results of the convolution to speed up training --- compared to previously used functions for example sigmoid functions --- without affecting the receptive fields of the convolution layer. The pooling layer also reduces the number of parameters and helps prevent over-fitting. The most common function to implement is \textit{max pooling}. The features are partitioned into a set of non-overlapping rectangles (if input is 2D) and each of these rectangles  is represented by its maximal value. 
The final layer is usually a fully connected. Its neurons have connections to all activations of the previous layer and their activation is then computed as an affine transformation. 

\subsection{Deep belief networks}
Deep belief networks introduced by Hinton \cite{Hinton504} are multiple \textit{Restricted Boltzmann machines} greedily stacked on top of each other. RBMs are shallow two-layer neural nets created by Paul Smolensky in 1986 \cite{Smolensky1986InformationPI}. The first layes is called the visible layer, the second layer is called the hidden layer. The nodes of each layer communicate with the previous and subsequent layer but there are no connections between nodes of the same layer. Each visible node takes a low level feature to be learned and multiplies it by some weight. The results for each feature of an input sample are then summed, bias is added and this is the result passed through an activation algorithm which produces output for each hidden node. These outputs then can be redirected into another hidden layer instead of the output of the neural network. 

RBMs also have the ability to reconstruct data without supervision. When the input makes it through both layers it then becomes input for the hidden layer and travels through the neural network in the opposite direction. The activations are multiplied by the same weights and passed to the visible layer where a new bias is added. The output of the visible layer is then compared to the initial input and the network adjusts weights so that it minimizes the difference between the input and the output.

\subsection{Recurrent neural networks}
Recurrent neural networks have one major difference compared to other neural networks. They include feedback loops in their structure which allows them to exhibit dynamic behaviour and makes them useful for processing sequential data. RNNs have a hidden state that is determined by previous states and is updated with every subsequent step. There are many variants of RNNs such as \textit{Fully recurrent, Long short-term memory} introduced by Hochreiter and Schmidhuber in \cite{doi:10.1162/neco.1997.9.8.1735}, \textit{Gated recurent units} first described in \cite{cho-etal-2014-learning}, or \textit{Bi-directional} invented by Schuster and Paliwal in \cite{Schuster1997BidirectionalRN}. 

Recurrent neural networks are used for working with sequential data for example in speech recognition \cite{DBLP:journals/corr/abs-1303-5778} or time-series anomaly detection \cite{inproceedings_RNN_anomaly_detection}.


\subsection{Autoencoders}
Autoencoders are not tied to one type of neural network. They can be build from recurrent layers, convolutional layers or using a simple Multi-layer Perceptron.

The autoencoder has two parts the encoder and the decoder. It is trained in an unsupervised manner. The encoder takes the input data and reduces its dimensions. The decoder then tries to recreate it into its original form without knowing, what the data looked like before the encoder processed it. The idea is, that over time, the encoder learns how to shrink the data with retaining as much information as possible for the decoder so the decoder it is able to recreate the original form as accurately as possible. 

\section{Related work}\label{sec:audio_related_work}
For the signal representation, there is obviously the possibility to use raw audio data as input for any machine learning algorithm. This however is a quite uncommon approach and when tested recently in tag prediction \cite{6854950} it had worse results than standard spectrograms. Multiple studies and experiments have been done using spectrograms as audio representation for classification tasks --- for example \cite{wang2014improving} --- or for unsupervised learning \cite{van2013deep}, \cite{Ramakrishnan2017song2V}, \cite{NIPS2009_3674}, which is what also our area of interest. 

In these studies the inputs were fed into various neural network. The output of those was then evaluated and compared to music classification or simply music similarity estimation using similarity measures directly on spectrograms. We are going to do a similar thing in this thesis. Our evaluation will be done on user playlists and the methods will be compared to each other.

\section{Audio implementation choices}

\subsection{Basic audio representation choices}
The flattened vectors from spectrograms, mel-spectrograms and MFCCs have tens sometimes even hundreds of thousands of features which is a big disadvantage especially when we have an intention of implementing them inside the web application. Nevertheless, we decided to test recommendations based at least on \textbf{raw mel-spectrograms} and \textbf{raw mfccs} to have an idea on how raw data behave. We also decided to use the \textbf{PCA} to transform spectrograms, mel-spectrograms and MFCCs because we believed that it could reduce their vector lengths to reasonable numbers which has shown to be true.


\subsection{Deep Audio representation choices}
Neural networks are a quickly developing and expanding field with many various applications. It is extremely time consuming to build an accurate neural network for specific task. Therefore, we decided to choose our neural network architecture and parameters based on literature relevant to the topic of audio processing. The main requirements we had for the algorithms was that they have to work in an unsupervised manner and that they have to reduce the feature space. This pruned the number of possibilities for us considerably, as most architectures are designed for music classification (mostly into genres) or speech and sound recognition. 

We decided that \textbf{autoencoders} suite the task in this thesis best. It is an unsupervised algorithm. The encoder part does exactly what we desire which is encoding the input sample into a smaller dimension. The decoder transforms it again into the same vector however, this part of the autoencoder is only necessary for training.

Since we are working with sound data, the choice was to use RNN layers which have the ability to encode sequences (spectrograms, mel-spectrograms and mfccs are 2D matrices). These choices were mainly inspired by this paper \cite{inproceedings_RNNs}. However we want to add more methods into comparison, so we implemented not only neural networks with \textbf{GRU} layers but also \textbf{LSTM} layers and used spectrograms, mel-spectrograms and also mfccs as input rather than focusing on one type of layer, input and tuning one particular neural network to give the best results possible. 